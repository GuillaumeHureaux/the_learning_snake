{"cells":[{"cell_type":"code","execution_count":1,"source":["import os\r\n","import sys\r\n","sys.path.append('..')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["from src.data.snake_ground import Ground\r\n","import numpy as np\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["class CustomEnvironment(Ground):\r\n","\r\n","    def __init__(self, x_max, y_max):\r\n","        super().__init__(x_max, y_max)\r\n","\r\n","    def states(self):\r\n","        # return {'ground': dict(type=\"float\", shape=(25, 25)),\r\n","        #         'face': dict(type=\"int\", shape=4,)\r\n","        # }\r\n","        return dict(type='float', shape=(25,25))\r\n","\r\n","    def actions(self):\r\n","        return dict(type='int', num_values=4)\r\n","\r\n","    # Optional: should only be defined if environment has a natural fixed\r\n","    # maximum episode length; otherwise specify maximum number of training\r\n","    # timesteps via Environment.create(..., max_episode_timesteps=???)\r\n","    def max_episode_timesteps(self):\r\n","        return 200\r\n","\r\n","    # Optional additional steps to close environment\r\n","    def close(self):\r\n","        super().close()\r\n","\r\n","    def reset(self):\r\n","        self.__init__(self.x_max, self.y_max)\r\n","        state = self.get_state()\r\n","        return state\r\n","\r\n","    def execute(self, actions):\r\n","        action = {1:'up', 2:'down', 3:'left', 0:'right'}[actions]\r\n","        reward = self.step(actions)\r\n","        next_state = self.get_state()\r\n","        terminal = bool(self.flag)\r\n","        print(reward)\r\n","        return next_state, terminal, reward\r\n","    \r\n","    def close(self):\r\n","        return len(self.snake)\r\n","\r\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["def run(environment, agent, n_episodes, max_step_per_episode, test=False):\n","    \"\"\"\n","    Train agent for n_episodes\n","    \"\"\"\n","    # environment.FlightModel.max_step_per_episode = max_step_per_episode\n","    # Loop over episodes\n","    for i in range(n_episodes):\n","        # Initialize episode\n","        episode_length = 0\n","        states = environment.reset()\n","        internals = agent.initial_internals()\n","        terminal = False\n","        while not terminal:\n","            # Run episode\n","            episode_length += 1\n","            actions = agent.act(states=states)\n","            states, terminal, reward = environment.execute(actions=actions)\n","            agent.observe(terminal=terminal, reward=reward)\n","\n","def runner(\n","    environment,\n","    agent,\n","    max_step_per_episode,\n","    n_episodes,\n","    n_episodes_test=1,\n","    combination=1,\n","):\n","    # Train agent\n","    result_vec = [] #initialize the result list\n","    for i in range(round(n_episodes / 100)): #Divide the number of episodes into batches of 100 episodes\n","        if result_vec:\n","            print(\"batch\", i, \"Best result\", result_vec[-1]) #Show the results for the current batch\n","        # Train Agent for 100 episode\n","        run(environment, agent, 100, max_step_per_episode) \n","        # Test Agent for this batch\n","        test_results = run(\n","                environment,\n","                agent,\n","                n_episodes_test,\n","                max_step_per_episode,\n","                test=True\n","            )\n","        # Append the results for this batch\n","        result_vec.append(test_results)\n","    print(result_vec)\n","    # Plot the evolution of the agent over the batches\n","    # plot_multiple(\n","    #     Series=[result_vec],\n","    #     labels = [\"Reward\"],\n","    #     xlabel = \"episodes\",\n","    #     ylabel = \"Reward\",\n","    #     title = \"Reward vs episodes\",\n","    #     save_fig=True,\n","    #     path=\"env\",\n","    #     folder=str(combination),\n","    #     time=False,\n","    # )\n","    #Terminate the agent and the environment\n","    agent.close()\n","    environment.close()\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["from tensorforce import Agent\n","\n","environment = CustomEnvironment(25, 25)\n","\n","# Instantiate a Tensorforce agent\n","agent = Agent.create(agent=\"ppo\",environment=environment, batch_size=10)\n","\n","# Call runner\n","runner(\n","    environment,\n","    agent,\n","    max_step_per_episode=1000,\n","    n_episodes=10000)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"orig_nbformat":4,"kernelspec":{"name":"python3","display_name":"Python 3.8.11 64-bit ('snake': conda)"},"interpreter":{"hash":"578b9149fed9dbee5818a03736f68609115ba468f632d923b6a946d371fe114f"}}}